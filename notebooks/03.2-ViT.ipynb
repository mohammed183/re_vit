{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the Vision Transformer\n",
    "\n",
    "In this notebook we use the pre-trained **ViT-L/16** model on the **ImageNet-21k** dataset which contains about 14 million images. The model will be finetuned on different datasets which are used for image classification tasks and then compared wiht the performance of the baseline model. We will use the pre-trained weights on **hugging face** which are the same weights provided by the authors but translated to be used in Pytorch.\n",
    "\n",
    "This notebook contains five datasets, you have to run the Utility Functions subsection then you can choose which datasets you want to evaluate the ViT model on by running its specific subsection. The following table contains the approximate running time for each dataset:\n",
    "\n",
    "|    Dataset     | Time (hh:mm:ss) |\n",
    "|:--------------:|:---------------:|\n",
    "|    Imagenet    |    00:40:00     |\n",
    "|    CIFAR-10    |    05:25:00     |\n",
    "|   CIFAR-100    |    05:25:00     |\n",
    "|  Oxford Pets   |    00:36:00     |\n",
    "| Oxford Flowers |    00:30:00     |\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "53b306ef-65db-46be-a91e-f73d67c22425"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "The following cells demonstrate the process of fine-tuning the **ViT-L/16** model. While this process is similar to that of the **ResNet** notebook, there are a few key differences to note:\n",
    "\n",
    "1.  The **dataloaders** employ distinct crop sizes and transforms\n",
    "\n",
    "2.  The **prediction** method differs due to the use of another model, but the **fine-tuning** function and **LR scheduler** remain the same\n",
    "\n",
    "3.  The **hyperparameters** have been adjusted to better suit the **ViT** model\n",
    "\n",
    "We start by importing the required modules:"
   ],
   "id": "9ec4de7e-918d-4d76-977e-cc57bf67a6b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from transformers import ViTForImageClassification\n",
    "from torchvision import transforms, datasets, models"
   ],
   "id": "6e22af58-f01c-4cc4-aca3-174df3726ce8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Similar to what we did with the **ResNet** model, we create a dataloader that loads the same five datasets. The difference between the two functions is the size of the images in the dataloader as each model tolerates different input sizes for the images. We use the same development split ratio and normalization.\n",
    "\n",
    "**The paper fine-tuned the model using an image resolution of 384x384. However, a resolution of 224x224 will be used, as it has been demonstrated through experiments to produce better results.**"
   ],
   "id": "6af2f72a-5109-4bb5-ab44-b561c5ce6629"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for transformer\n",
    "def get_vit_loaders(dataset=\"imagenet\", batch_size=64):\n",
    "    \"\"\"\n",
    "    This loads the whole dataset into memory and returns train and test data to\n",
    "    be used by the Vision Transformer\n",
    "    @param dataset (string): dataset name to load\n",
    "    @param batch_size (int): batch size for training and testing\n",
    "\n",
    "    @returns dict() with train, val and test data loaders with keys `train_loader`, `val_loader` and `test_loader`\n",
    "    \"\"\"\n",
    "    # Normalization using channel means\n",
    "    normalize_transform = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "    # Creating transform function\n",
    "    train_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), normalize_transform])\n",
    "\n",
    "    # Test transformation function\n",
    "    test_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), normalize_transform])\n",
    "\n",
    "    # Loader arguments\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "\n",
    "    # Load the dataset from torchvision datasets\n",
    "    if dataset == \"imagenet\":\n",
    "        # Create Special Test transform\n",
    "        test_transform = transforms.Compose([transforms.Resize((384,384)), transforms.ToTensor(), normalize_transform])\n",
    "        # Load the dataset\n",
    "        original_test_dataset = datasets.ImageFolder(root=os.path.join('data', 'imagenet', 'val'), transform=test_transform)\n",
    "        # Create Data Loader\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset=original_test_dataset,\n",
    "            shuffle=True,\n",
    "            **loader_args)\n",
    "        # Return Test Loader\n",
    "        return {\"test_loader\": test_loader}\n",
    "\n",
    "    elif dataset == \"cifar10\":\n",
    "        # Load CIFAR-10\n",
    "        original_train_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == \"cifar100\":\n",
    "        # Load CIFAR-100\n",
    "        original_train_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == \"oxford_pets\":\n",
    "        # Load Oxford-IIIT Pets\n",
    "        original_train_dataset = datasets.OxfordIIITPet(root=os.path.join('data', 'oxford_iiit_pets_data'),\n",
    "                                             split='trainval', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.OxfordIIITPet(root=os.path.join('data', 'oxford_iiit_pets_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    elif dataset == \"flowers_102\":\n",
    "        # Load Oxford Flowers-102\n",
    "        original_train_dataset = datasets.Flowers102(root=os.path.join('data', 'oxford_flowers_102_data'),\n",
    "                                             split='train', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.Flowers102(root=os.path.join('data', 'oxford_flowers_102_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    else:\n",
    "        # Raise an error if the dataset is not valid\n",
    "        raise ValueError(\"Invalid dataset name. Please choose one of the following: imagenet, cifar10, cifar100, oxford_pets, flowers_102\")\n",
    "\n",
    "\n",
    "    # Set the validation set size\n",
    "    val_size = int(0.02 * len(original_train_dataset)) if dataset in ['cifar10', 'cifar100'] else int(0.1 * len(original_train_dataset))\n",
    "\n",
    "    # Set train size as remaining data\n",
    "    train_size = len(original_train_dataset) - val_size\n",
    "\n",
    "    # Split the original train dataset into train and validation datasets\n",
    "    train_dataset, val_dataset = random_split(original_train_dataset, [train_size, val_size])\n",
    "\n",
    "    # Create the data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_test_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    return {\"train_loader\": train_loader,\n",
    "            \"val_loader\": val_loader,\n",
    "            \"test_loader\": test_loader}"
   ],
   "id": "03b4bb70-dc66-4b58-a6ef-8a8165d21c26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "-   `get_accuracy`: This function takes the model predictions and the true labels as inputs and returns the accuracy as a float value.\n",
    "-   `evaluate_on_test`: This function takes the model, the loss criterion, the test dataloader, and the device as inputs and returns the test accuracy and loss as float values."
   ],
   "id": "1a4ae35a-fb7f-4e63-8dca-031245839d97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes predictions and true values to return accuracies\n",
    "def get_accuracy(logit, true_y):\n",
    "    pred_y = torch.argmax(logit, dim=1)\n",
    "    return (pred_y == true_y).float().mean()\n",
    "\n",
    "# This Function is used to evaluate the model\n",
    "def evaluate_on_test(model, test_loader, device=\"cpu\"):\n",
    "    # Evaluate the model on all the test batches\n",
    "    accuracies = []\n",
    "    model.eval()\n",
    "    for batch_idx, (data_x, data_y) in enumerate(test_loader):\n",
    "        data_x = data_x.to(device)\n",
    "        data_y = data_y.to(device)\n",
    "\n",
    "        model_y = model(data_x)\n",
    "        batch_accuracy = get_accuracy(model_y.logits, data_y)\n",
    "\n",
    "        accuracies.append(batch_accuracy.item())\n",
    "\n",
    "        if batch_idx%1000 == 0:\n",
    "            print(f\"Mean accuracy at batch: {batch_idx} is {np.mean(accuracies) * 100}\")\n",
    "\n",
    "    test_accuracy = np.mean(accuracies) * 100\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "    return test_accuracy"
   ],
   "id": "903992e1-912b-463b-8bb3-c10b6b22d3c1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following function is used to train a vision transformer model on a given dataset and evaluate its performance on the train and test sets. The function takes the following arguments:\n",
    "\n",
    "-   `loaders`: a dictionary of PyTorch dataloaders for the train, test, and validation sets.\n",
    "-   `model_name`: a string to specify the name of the vision transformer model to use. The default is â€˜google/vit-base-patch16-224-in21kâ€™, which is a pre-trained model from Google that uses 16x16 patches and has 224 hidden units.\n",
    "-   `lr`: a float to specify the learning rate for the optimizer. The default is 0.001.\n",
    "-   `epochs`: an integer to specify the number of epochs to train the model. The default is 10.\n",
    "\n",
    "The function returns two lists of floats, which are the train and test accuracies for each epoch."
   ],
   "id": "c01e07bc-0481-4e9c-b7e5-a5fa650147cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model and return train and test accuracies\n",
    "def train_vit_model(loaders, title=\"\", model_name='google/vit-large-patch16-224-in21k',\n",
    "                         lr=0.001, epochs=10, random_seed=42, save=False):\n",
    "\n",
    "    # Create experiment directory\n",
    "    experiment_dir = os.path.join('experiments', title)\n",
    "\n",
    "    # make experiment directory\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "    # Set the seed\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Get num_classes\n",
    "    num_classes = len(loaders[\"train_loader\"].dataset.dataset.classes)\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    model = ViTForImageClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    # Move the model to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Calculate per_step, authors used 512 batch size\n",
    "    batch_size = loaders[\"train_loader\"].batch_size\n",
    "    T_max = len(loaders[\"train_loader\"]) * epochs\n",
    "\n",
    "    # Create the scheduler with cosine learning rate decay\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.to(device);\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "\n",
    "        # Calculate loss and gradients for models on every training batch\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"train_loader\"]):\n",
    "            data_x = data_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model_y = model(data_x).logits\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "            # Perform back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # Get this epoch accuracy and loss\n",
    "        train_loss = np.mean(losses)\n",
    "        train_accuracy = np.mean(accuracies)*100\n",
    "        print(\"Train accuracy: {} Train loss: {}\".format(train_accuracy, train_loss))\n",
    "\n",
    "        # Evaluate the model on all the Validation batches\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        # Move the model to CPU\n",
    "        model.to(\"cpu\")\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"val_loader\"]):\n",
    "            # Move the data to CPU\n",
    "            data_x = data_x.to(\"cpu\")\n",
    "            data_y = data_y.to(\"cpu\")\n",
    "\n",
    "            model_y = model(data_x).logits\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Print Validation accuracy and loss\n",
    "        val_loss = np.mean(losses)\n",
    "        val_accuracy = np.mean(accuracies)*100\n",
    "        print(\"Validation accuracy: {} Validation loss: {}\".format(val_accuracy, val_loss))\n",
    "\n",
    "    # Save the final model\n",
    "    if save:\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, os.path.join(experiment_dir, f'Vit-L/16{title}.pt'))\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_acc = evaluate_on_test(model, loaders['test_loader'])\n",
    "\n",
    "    # Delete the data and model outputs from GPU memory\n",
    "    del model\n",
    "    # Release unused memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # return the accuracies\n",
    "    return test_acc"
   ],
   "id": "8a49339e-df30-4b65-a3c9-cdb81070515f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following function `plot_images_from_dataloader` takes a PyTorch dataloader as an input and plots 10 of the images from the first batch of data. The function also shows the labels of the images according to the classes attribute of the dataloaderâ€™s dataset. There is a second function to print the time taken by the model."
   ],
   "id": "a874983d-d7bf-4fb0-bcf1-b53346d751fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot 10 of the images\n",
    "def plot_images_from_dataloader(dataloader, classes=None):\n",
    "    # Initialize empty tensors for images and labels\n",
    "    images = torch.empty(0)\n",
    "    labels = torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    # Loop until the images and labels have at least 10 elements\n",
    "    while len(images) < 10:\n",
    "        # Get the next batch of images and labels from the dataloader\n",
    "        batch_images, batch_labels = next(iter(dataloader))\n",
    "        # Concatenate the batch images and labels to the existing tensors\n",
    "        images = torch.cat((images, batch_images), dim=0)\n",
    "        labels = torch.cat((labels, batch_labels), dim=0)\n",
    "    # Get class names\n",
    "    if classes is None:\n",
    "        classes = dataloader.dataset.dataset.classes\n",
    "    # Create a figure with 2 rows and 5 columns\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = images[i]\n",
    "        label = classes[labels[i]]\n",
    "        # Unnormalize the image\n",
    "        image = image / 2 + 0.5\n",
    "        image = image.numpy()\n",
    "        # Transpose the image\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        # Plot the image on the axis\n",
    "        ax.imshow(image)\n",
    "        # Set title as label\n",
    "        ax.set_title(label)\n",
    "        # Turn off the axis ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to calculate runtime per dataset\n",
    "def print_time(start_time, end_time):\n",
    "    # Calculate the difference in seconds\n",
    "    diff = end_time - start_time\n",
    "\n",
    "    # Convert the difference to hours, minutes, and seconds\n",
    "    hours, remainder = divmod(diff, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    # Create time in hours:minutes:seconds format\n",
    "    time_string = f\"{int(hours)}:{int(minutes)}:{seconds}\"\n",
    "\n",
    "    # Print the time\n",
    "    print(f\"Cell execution time: {time}\")\n",
    "\n",
    "    return time_string"
   ],
   "id": "bfbe5777-de92-4853-a078-4cabf8fb6830"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "3e5b2667-f724-465c-bcac-17cf4bb0bf05"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size chosen in the following runs are for a `RTX6000 GPU`. If you are using another GPU you can try changing it to maximize the usage of your GPU.\n",
    "\n",
    "You can use any of these commands in the terminal while running the cells to check how much of your GPU memory is utilized:\n",
    "\n",
    "-   `nvidia-smi --query-gpu=gpu_name,memory.used,memory.free,memory.total --format=csv`\n",
    "-   `nvidia-smi pmon -s m -c 1`\n",
    "\n",
    "**Note that batch size used in the original paper was 512 samples per batch**\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "f3c32432-5d56-4242-8ca1-d2d487c8dbf1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet\n",
    "\n",
    "The ImageNet dataset consists of **1000** object classes and contains **1,281,167** training images, **50,000** validation images and **100,000** test images. The images vary in resolution but it is common practice to train deep learning models on sub-sampled images of **256x256** pixels. This dataset is widely used for image classification and localization tasks and has been the benchmark for many state-of-the-art algorithms.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "27bac6d9-7fde-4190-95ab-40cdef577a04"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is load the dataset into a PyTorch dataloader and plot a sample of 10 images to visualize it."
   ],
   "id": "b5f5b006-3243-4766-9602-e36d12fce961"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the ImageNet dataset\n",
    "loader = get_vit_loaders(dataset=\"imagenet\", batch_size=4)\n",
    "plot_images_from_dataloader(loader[\"test_loader\"], loader[\"test_loader\"].dataset.classes)"
   ],
   "id": "eb146de5-592d-4309-a915-a8b037667222"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We load the model that was pre-trained and fine-tuned on the **ImageNet-1k** which was published and evaluate the model directly."
   ],
   "id": "ad15fc2e-dd6e-41f6-b107-9cbaf609d290"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"CUDA Recognized\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Get the fine tuned model on the ImageNet dataset\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-large-patch16-384')\n",
    "# Move the model to the device\n",
    "model = model.to(device)"
   ],
   "id": "861b01bc-3055-4a12-bf1a-7a87dcc09f31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We evaluate the model on the 50k validation samples to measure the modelâ€™s performance."
   ],
   "id": "bec09bee-edd9-442d-9f65-4a28b0a8b4d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Print the Performance of the Ready fine tuned model\n",
    "test_acc_imagenet = evaluate_on_test(model, loader[\"test_loader\"], device)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "imagenet_time = print_time(start_time, end_time)\n",
    "# delete model to free gpu\n",
    "del model\n",
    "# Release unused memory\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "ffef76fb-dcb9-4489-b04d-e68b88bfedaa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the results for the **ImageNet** dataset in a dictionary to use later for comparing results."
   ],
   "id": "81330e58-e625-4351-8d6d-1af6e6b96306"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/vit.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/vit.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"imagenet\"] = test_acc_imagenet   \n",
    "\n",
    "# Create dictionary times\n",
    "times = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/vit_time.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/vit_time.json\", \"r\") as f:\n",
    "        # Load the data from the file to times\n",
    "        times = json.load(f)\n",
    "\n",
    "# Add the time to a dictionary\n",
    "times[\"imagenet\"] =  imagenet_time                "
   ],
   "id": "7746b3b1-36d4-4aeb-98f9-968390b19c0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/vit.json\", \"w\") as f:\n",
    "    json.dump(runs, f)\n",
    "\n",
    "with open(\"experiments/vit_time.json\", \"w\") as f:\n",
    "    json.dump(times, f)"
   ],
   "id": "045629fa-e1ec-495f-ad9d-d3a3e1916c79"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "abc7ef23-3e76-4b2d-af74-62d69e207cf3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of **60,000 32x32** color images in **10** different classes. The 10 classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. There are **6,000** images per class, with **5,000** for training and **1,000** for testing. It is a popular benchmark for image classification and deep learning research.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "e82b891b-b767-41ce-a32a-b1609c8abf20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the dataset into dataloaders and plot a sample to visualize how the dataset looks like. **Note** that you should decide the `batch_size` that fits the GPU you are using."
   ],
   "id": "623dd59d-b9a5-4e64-82d9-5a42e04b05af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the CIFAR-10 dataset\n",
    "loader = get_vit_loaders(dataset=\"cifar10\", batch_size=32)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "a01c3dd1-d911-4212-b62b-a6061613f9b3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We fine tune the pre-trained vision transformer on the **CIFAR-10** dataset to get the train and test accuracies."
   ],
   "id": "71502045-2ff3-4e72-a1f2-f6d32c773294"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on CIFAR-10\n",
    "test_acc_cifar10 = train_vit_model(loaders=loader)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "cifar10_time = print_time(start_time, end_time)"
   ],
   "id": "3ee6c25b-a230-427a-bc6f-a45cb16c6bc6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the obtained results to use it later for the creating the results table."
   ],
   "id": "c55f4952-a1f6-4bcd-8fbd-aebd8f323658"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/vit.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/vit.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"cifar10\"] = test_acc_cifar10\n",
    "\n",
    "# Create dictionary times\n",
    "times = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/vit_time.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/vit_time.json\", \"r\") as f:\n",
    "        # Load the data from the file to times\n",
    "        times = json.load(f)\n",
    "\n",
    "# Add the time to a dictionary\n",
    "times[\"cifar10\"] =  cifar10_time "
   ],
   "id": "39773084-7a83-499b-ad68-7e3eb9748d83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/vit.json\", \"w\") as f:\n",
    "    json.dump(runs, f)\n",
    "\n",
    "with open(\"experiments/vit_time.json\", \"w\") as f:\n",
    "    json.dump(times, f)"
   ],
   "id": "d185caba-7ad2-4389-9c2e-cfcea5752cc1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "ea022c4d-c28f-49cd-874e-c25214861a3c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-100\n",
    "\n",
    "The CIFAR-100 dataset consists of **60,000 32x32** color images in **100** different classes. The 100 classes are grouped into 20 superclasses, such as aquatic mammals, flowers, insects, vehicles, etc. There are **600** images per class, with **500** for training and **100** for testing. It is also a commonly benchmark for image classification and deep learning research.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "02b44d0d-3534-4aa7-b92f-23dc227ef443"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset into a PyTorch dataloader and plot the first 10 images from the train loader. **Note** that we need to make sure that the batch size is greater than or equal 10 to prevent any errors during plotting."
   ],
   "id": "7c9c9380-994e-42a9-b677-1061b226c451"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the CIFAR-100 dataset\n",
    "loader = get_vit_loaders(dataset=\"cifar100\", batch_size=32)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "aab3086d-1f1f-441c-bee7-245418f0b825"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Now we are ready to fine tune the model on the **CIFAR100** dataset."
   ],
   "id": "a2f887c5-37de-4a0d-b13d-9d3ffc832864"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on CIFAR-100\n",
    "test_acc_cifar100 = train_vit_model(loaders=loader)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "cifar100_time = print_time(start_time, end_time)"
   ],
   "id": "dd91f99f-0f37-46da-9dc4-7f79b910e845"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the results in `runs` using the key `cifar100`."
   ],
   "id": "8ea33946-96ee-44ea-8d20-c8b86f7ac152"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/vit.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/vit.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"cifar100\"] = test_acc_cifar100\n",
    "\n",
    "# Create dictionary times\n",
    "times = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/vit_time.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/vit_time.json\", \"r\") as f:\n",
    "        # Load the data from the file to times\n",
    "        times = json.load(f)\n",
    "\n",
    "# Add the time to a dictionary\n",
    "times[\"cifar100\"] =  cifar100_time "
   ],
   "id": "ce3d4dec-db22-4a4e-9024-d876bed61b57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/vit.json\", \"w\") as f:\n",
    "    json.dump(runs, f)\n",
    "\n",
    "with open(\"experiments/vit_time.json\", \"w\") as f:\n",
    "    json.dump(times, f)"
   ],
   "id": "37ff9015-be4e-4edb-bde3-1a2c0191255a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "bbce54eb-000a-482b-9a80-3a2149cf0870"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxford-IIIT Pets\n",
    "\n",
    "The Oxford-IIIT Pets is a **37** category pet dataset with roughly **200** images for each class created by the Visual Geometry Group at Oxford. The images have large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI (region of interest), and pixel level trimap segmentation. The dataset is useful for fine-grained image classification and segmentation tasks.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "d3d69cff-7e21-4c6a-ac6d-5b41c53d7c40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the dataset and plot a random sample of 10 images from the train loader."
   ],
   "id": "91f4a770-d615-44a4-9a1c-49e9eb294b65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the Oxford-IIIT Pets dataset\n",
    "loader = get_vit_loaders(dataset=\"oxford_pets\", batch_size=32)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "6537a9ce-e3f3-412a-9c12-91e4630f7c14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We fine-tune the vision transformer on the dataset for 10 epochs."
   ],
   "id": "e8784a5c-11fe-4dae-815f-f8a8e93539b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on Oxford-IIIT Pets\n",
    "test_acc_oxford_pets = train_vit_model(loaders=loader, lr=0.003)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "oxford_pets_time = print_time(start_time, end_time)"
   ],
   "id": "56427595-a5a1-41af-a547-c900f2ae7f1a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the results in the `runs` dictionary as usual using the dataset name as key."
   ],
   "id": "99e6bc16-c49e-448f-bf8d-546df76aac80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/vit.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/vit.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"oxford_pets\"] = test_acc_oxford_pets\n",
    "\n",
    "# Create dictionary times\n",
    "times = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/vit_time.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/vit_time.json\", \"r\") as f:\n",
    "        # Load the data from the file to times\n",
    "        times = json.load(f)\n",
    "\n",
    "# Add the time to a dictionary\n",
    "times[\"oxford_pets\"] =  oxford_pets_time "
   ],
   "id": "2c035d4f-9ced-4211-a6ca-7329d748bd48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/vit.json\", \"w\") as f:\n",
    "    json.dump(runs, f)\n",
    "\n",
    "with open(\"experiments/vit_time.json\", \"w\") as f:\n",
    "    json.dump(times, f)"
   ],
   "id": "19c157db-baba-4c3a-9f44-eec8052f6e29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "cf428e32-ecfb-4a21-9f77-dff762d081ac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxford Flowers-102\n",
    "\n",
    "The Oxford Flowers-102 dataset consists of **102** flower categories commonly occurring in the United Kingdom. Each class consists of between **40 and 258** images. The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories. The dataset also provides image labels, segmentations, and distances based on shape and color features.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "ab448b2d-229e-4f86-9ede-888c0a3e1eb6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset we need to define the classes array as it is not defined in the PyTorch dataset. We load the dataset into PyTorch data loader and plot the first 10 images."
   ],
   "id": "6f16726c-7102-4dc2-bb17-1430393a6f8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the Oxford Flowers-102 Pets dataset\n",
    "loader = get_vit_loaders(dataset=\"flowers_102\", batch_size=32)\n",
    "\n",
    "# We initialize the flowers names as they are not on Pytorch (used for plotting)\n",
    "flower_classes = ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',\n",
    " 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle',\n",
    " 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower',\n",
    " 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary',\n",
    " 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke',\n",
    " 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly',\n",
    " 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy',\n",
    " 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy',\n",
    " 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura',\n",
    " 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan',\n",
    " 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy',\n",
    " 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily',\n",
    " 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia',\n",
    " 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea',\n",
    " 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']\n",
    "\n",
    "# Save the Class names in the dataset\n",
    "loader[\"train_loader\"].dataset.dataset.classes = flower_classes\n",
    "# Plot dataset\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "7e48468b-a908-4f0c-9699-22f907c33f42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We fine-tune the model using the `train_vit_model` function and using the `Flowers102` dataset loader as the input arguement. We change the number of epochs to 14 to match the number of epoch for the **ResNet** model."
   ],
   "id": "bc325a63-9de5-49e0-b185-a6b5bffbfe7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on Oxford Flowers-102\n",
    "test_acc_oxford_flowers = train_vit_model(loaders=loader, epochs=14, lr=0.003)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "oxford_flowers_time = print_time(start_time, end_time)"
   ],
   "id": "01f0b8da-b496-463f-8149-075cad80e829"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Now we save the fine-tuning results in the `runs` dictionary."
   ],
   "id": "9c9ace27-d65d-4cfe-a046-3ac54c11aba7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/vit.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/vit.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"oxford_flowers\"] = test_acc_oxford_flowers\n",
    "\n",
    "# Create dictionary times\n",
    "times = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/vit_time.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/vit_time.json\", \"r\") as f:\n",
    "        # Load the data from the file to times\n",
    "        times = json.load(f)\n",
    "\n",
    "# Add the time to a dictionary\n",
    "times[\"oxford_flowers\"] =  oxford_flowers_time "
   ],
   "id": "73127d98-0a29-454b-b25a-c3b500277729"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/vit.json\", \"w\") as f:\n",
    "    json.dump(runs, f)\n",
    "\n",
    "with open(\"experiments/vit_time.json\", \"w\") as f:\n",
    "    json.dump(times, f)"
   ],
   "id": "8ce6f497-11ea-4056-a447-971293a68ebf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "**In the ResNet notebook, we encountered several challenges that we had to overcome. Can you review the information in this notebook and identify the challenges we faced and how we addressed them? ðŸ§**"
   ],
   "id": "9c87014a-5bed-484c-9a5c-152f5e21c54c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We can now compare the results of the **Vision Transformer** pre-trained on the **ImageNet-21k** dataset from the paper with our results to verify the **quantitative** claim made by the authors. Please fill in the following table template:\n",
    "\n",
    "|        Model        | Imagenet | CIFAR-10 | CIFAR-100 | Oxford Pets | Oxford Flowers |\n",
    "|:------------------:|:-------:|:-------:|:--------:|:----------:|:-------------:|\n",
    "| Authorsâ€™ ViT (I21k) |          |          |           |             |                |\n",
    "|   Our ViT (I21k)    |          |          |           |             |                |\n",
    "|     Difference      |          |          |           |             |                |\n",
    "\n",
    "**Were we able to verify the quantitative results for this model? ðŸ§**"
   ],
   "id": "fffd3189-9441-470b-a1b9-d28e6ce439f7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Now that we are done with both the **ResNet** and the **ViT** model, we can create the results table back at the **Experiments** notebook."
   ],
   "id": "99d60a9b-f15e-4606-923b-f8559c325b1b"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
