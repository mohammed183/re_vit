{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claims üìù\n",
    "\n",
    "The paper ‚ÄúAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale‚Äù evaluates the performance of **ResNet**, **ViT**, and **hybrid** models on various image classification tasks and makes several claims based on their assessment. In this section, we present these claims and propose a way to test them using the **pre-trained** models provided by the authors. The table below, taken from the paper, shows the different variants of the models used by the authors, who also use a specific notation to refer to the models. For example, ‚ÄúViT-L/16‚Äù refers to the ‚ÄúLarge‚Äù variant with an input patch size of 16x16. The authors also use an improved ResNet as a baseline model, referred to as [‚ÄúResNet (BiT)‚Äù](https://arxiv.org/abs/1912.11370), which has published models available on [GitHub](https://github.com/google-research/big_transfer).\n",
    "\n",
    "|   Model   | Layers | Hidden size D | MLP size | Heads | Params |\n",
    "|:---------:|:------:|:-------------:|:--------:|:-----:|:------:|\n",
    "| ViT-Base  |   12   |      768      |   3072   |  12   |  86M   |\n",
    "| ViT-Large |   24   |     1024      |   4096   |  16   |  307M  |\n",
    "| ViT-Huge  |   32   |     1280      |   5120   |  16   |  632M  |\n",
    "\n",
    "We aim to evaluate the claims made in the Vision Transformer paper both qualitatively and quantitatively. However, we face several **challenges** in doing so:\n",
    "\n",
    "-   Some of the **pre-trained** models used by the authors are not publicly available\n",
    "\n",
    "-   We do not have sufficient **computational** resources to train the models from scratch\n",
    "\n",
    "-   Some of the datasets used for pretraining are private and inaccessible\n",
    "\n",
    "As a result, we may not be able to reproduce all of the results presented in the paper. We will carefully examine each claim to determine which parts can be reproduced and which cannot be reproduced given the challenges associated with that claim.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "22add8ae-2e38-463a-96b0-176fb013bdb9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Claim: Vision Transformer outperforms state of the art CNNs on various classification tasks after pretraining on large datasets\n",
    "\n",
    "This claim suggests that the vision transformer can leverage more knowledge from large datasets than CNN models during pretraining and transfer this knowledge to the fine tuning task. This implies that the vision transformer can achieve higher or comparable accuracies to the state of the art models on different classification tasks.\n",
    "\n",
    "The authors support their claim by pretraining three versions of the vision transformer and evaluating their performance on various benchmarks. However, they use the **JFT-300M** dataset for pretraining, which is a private dataset owned by Google and *not available* to the public. Additionally, the authors do not share the models pre-trained on the **JFT-300M** dataset. The only explicit comparison presented in Table 2 of the paper is between Vision Transformers and CNNs pre-trained on the same private **JFT-300M** dataset. The authors also show results for a Vision Transformer pre-trained on the publicly available **ImageNet-21k** dataset but not compared with a CNN pre-trained on the same dataset. It is worth noting that *BiT-L is a ResNet152x4 model*, which is a type of CNN. The following table shows a subset of the results mentioned above:\n",
    "\n",
    "<table style=\"width: 100%;\">\n",
    "<tr>\n",
    "<th style=\"text-align: center;\">\n",
    "Model\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "ImageNet\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "ImageNet ReaL\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "CIFAR-10\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "CIFAR-100\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "Oxford-IIIT Pets\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "Oxford Flowers-102\n",
    "</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">\n",
    "ViT-H/14 (JFT)\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "88.55\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "90.72\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "99.50\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "94.55\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "97.56\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "99.68\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">\n",
    "ViT-L/16 (JFT)\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "87.76\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "90.54\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "99.42\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "93.90\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "97.32\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "99.74\n",
    "</td>\n",
    "</tr>\n",
    "<tr style=\"background-color: lightgreen;\">\n",
    "<td style=\"text-align: center;\">\n",
    "ViT-L/16 (I21k)\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "85.30\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "88.62\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "99.15\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "93.25\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "94.67\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "99.61\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">\n",
    "BiT-L (JFT)\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "87.54\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "90.54\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "99.37\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "93.51\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "96.62\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "99.63\n",
    "</td>\n",
    "</tr>\n",
    "<tr style=‚Äúwhite-space: nowrap;‚Äù>\n",
    "        <td style=\"text-align: center;\">Noisy Student</td>\n",
    "        <td style=\"text-align: center;\">88.4</td>\n",
    "        <td style=\"text-align: center;\">90.55</td>\n",
    "        <td style=\"text-align: center;\"> - </td>\n",
    "        <td style=\"text-align: center;\"> - </td>\n",
    "        <td style=\"text-align: center;\"> - </td>\n",
    "        <td style=\"text-align: center;\"> - </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "To test the claim that vision transformers outperform convolutional neural networks on image classification tasks, we need to fine-tune the pre-trained models on various datasets and compare their test accuracy. However, we face a challenge: we cannot access the pre-trained models or the **JFT-300M** dataset that the authors used for pretraining. This is a private dataset that only they have. The only model we can use from the previous table is the **Vit-L/16 (I21k)**, which is marked in green. This model was pre-trained on the **ImageNet-21k (I21k)** dataset and released by Google.\n",
    "\n",
    "**How can we overcome this challenge and verify their claim**‚ÅâÔ∏è\n",
    "\n",
    "One possible solution to evaluate the performance of the Vision Transformer model is to use the pre-trained model on the **ImageNet-21k** dataset, which the authors have published results for, and compare it with the same CNN (BiT-L) but pre-trained on the same dataset which is we will refer to as BiT-M (provided by authors). We can fine-tune these models on the same classification datasets as the vision transformer model and compare their performance. We can then fine-tune the BiT-M on **ImageNet-1k**, **CIFAR-10**, **CIFAR-100**, **Oxford-IIIT Pets** and **Oxford Flowers-102** and use it as a baseline. Then fine-tune the ViT-L/16 (I21k) on the same datasets and compare the results with the baseline model. We can then create a table similar to the following with our results and we can see which quantitative results from the paper we are able to reproduce.\n",
    "\n",
    "<table style=\"width: 100%;\">\n",
    "<tr>\n",
    "<th style=\"text-align: center;\">\n",
    "Model\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "ImageNet\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "ImageNet ReaL\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "CIFAR-10\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "CIFAR-100\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "Oxford-IIIT Pets\n",
    "</th>\n",
    "<th style=\"text-align: center;\">\n",
    "Oxford Flowers-102\n",
    "</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">\n",
    "ViT-L/16 (I21k)\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "85.30\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "88.62\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "99.15\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "93.25\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "94.67\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "99.61\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">\n",
    "BiT-M (I21k)\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "?\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "?\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "?\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "?\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "?\n",
    "</td>\n",
    "<td style=\"text-align: center;\">\n",
    "?\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "2d293d55-ae3f-48a0-8338-3fffdb3d2a9d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the information provided, can you explain which parts of the authors‚Äô primary claim can and cannot be evaluated? Additionally, can you describe how we can evaluate the claims that are possible to evaluate? ü§î**\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "27d39279-91eb-49af-98af-8ed7b5817bd9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üõë There are other claims about the computational improvement of the vision transformer model compared to the traditional CNNs that achieve similar results. We will not address any of these claims due to their high computational cost. Moreover, some of these results are not reproducible because of the unavailability of the JFT-300M dataset.**\n",
    "\n",
    "**Can you identify one of these claims and provide the experiment they did to support their claim? üßê**\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "04fe060a-24b7-4ae4-9aba-921bd2ca7e83"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other claims about the performance of the Vision Transformer can be found in the [Going Further notebook](05-Going_Further.ipynb) with the experiments used to verify these claims and explanation for the challenges that we might face while verifying them.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "79dbfe23-2818-44a5-85e2-84d07a2a0cca"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
